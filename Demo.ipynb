{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b0432e-d309-469d-9f99-e5567d7f2e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyFiles\\AI\\Personal Projects\\EmbeddingFineTuning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerModelCardData, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97142371-80ac-4255-a641-66a67acf3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain, langchain_core, langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27b46555-77ec-426f-8bf6-1a47cfa506bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.76'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_core.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2d0d1f-1c30-4ee4-affe-77ec9c9c9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of CUDA devices: 1\n",
      "Current CUDA device name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5a318-4b2e-48f6-929e-eaa76778e97c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af7e213-9a84-4c36-a95b-28b70728bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DOCUMENTS_FOLDER = \"data/raw/\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "QUESTIONS_PER_CHUNK = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de7071-7bcb-4c33-83fd-fd276ff8efc2",
   "metadata": {},
   "source": [
    "#### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe33025-477a-438a-9e7a-47141e8c2126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86593a38-f8fc-49a0-9042-50f638eec162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LLM for question generation...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Setting up LLM for question generation...\")    \n",
    "    # LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=1,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up the OpenAI model: {e}\")\n",
    "    print(\"Please ensure you have `langchain-openai` installed and your OPENAI_API_KEY is set.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68a0095-1cd5-4a00-bb43-7a189b85c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "label_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant tasked with generating a single, realistic question-answer pair based on a given document. The question should be something a user might naturally ask when seeking information contained in the document.\n",
    "\n",
    "Given: {chunk}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the key topics, facts, and concepts in the given document, choose one to focus on.\n",
    "2. Generate 3 similar questions that a user might ask to find the information in this document that does NOT contain any company name.\n",
    "3. Use natural language and occasionally include typos or colloquialisms to mimic real user behavior in the question.\n",
    "4. Ensure the question is semantically related to the document content WITHOUT directly copying phrases.\n",
    "5. Make sure that all of the questions are similar to eachother. I.E. All asking about a similar topic/requesting the same information.\n",
    "\n",
    "Output Format:\n",
    "Return a JSON object with the following structure:\n",
    "```json\n",
    "{{\n",
    "  \"question_1\": \"Generated question text\",\n",
    "  \"question_2\": \"Generated question text\",\n",
    "  ...\n",
    "}}\n",
    "```\n",
    "\n",
    "Be creative, think like a curious user, and generate your 3 similar questions that would naturally lead to the given document in a semantic search. Ensure your response is a valid JSON object containing only the questions.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba55f50-5996-41fc-9808-7ea9e99830be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a specified number of questions for a given text chunk using an OpenAI model.\n",
    "    \"\"\"\n",
    "    # Use the model to generate the questions\n",
    "\n",
    "    label_chain = label_prompt | llm | JsonOutputParser()\n",
    "    response = label_chain.invoke({\"chunk\":chunk})\n",
    "    \n",
    "    # The response content is a single string. We'll split it into a list of questions.\n",
    "    questions = list(response.values())\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd32d2-93bf-4c8a-9e94-cc6c8b243cab",
   "metadata": {},
   "source": [
    "#### Documents processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8012898-ef53-4caf-96c0-ddb5558fef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1 document(s).\n"
     ]
    }
   ],
   "source": [
    "# 1. Load documents from the specified folder\n",
    "loader = DirectoryLoader(DOCUMENTS_FOLDER, glob=\"*.txt\",\n",
    "                         loader_cls=lambda path: TextLoader(path, encoding=\"utf-8\"))\n",
    "\n",
    "try:\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(f\"No '.txt' files found in the '{DOCUMENTS_FOLDER}' folder.\")\n",
    "    print(f\"Successfully loaded {len(documents)} document(s).\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0fe89e-7f73-41c8-81bb-1743c17b1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks...\n",
      "Created 191 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 2. Chunk the documents using the RecursiveCharacterTextSplitter\n",
    "print(\"Splitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa5a92-7575-4631-ac13-34549303d423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. For each chunk, create 2-5 questions using the LLM\n",
    "print(\"Generating questions for each chunk...\")\n",
    "dataset_list = []\n",
    "for i, chunk in enumerate(tqdm(chunks)):\n",
    "    try:\n",
    "        questions = generate_questions(chunk.page_content)\n",
    "        \n",
    "        # 4. Store the questions and chunks in the desired format\n",
    "        # Each chunk is paired with each of its generated questions\n",
    "        for q in questions:\n",
    "            dataset_list.append({\n",
    "                \"chunk_id\": f\"chunk_{i+1}\",\n",
    "                \"chunk\": chunk.page_content,\n",
    "                \"question\": q\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping chunk {i+1} due to an error during question generation: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ade24cd4-2347-4d46-ad84-82be4e65dfcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Hugging Face Dataset...\n",
      "\n",
      "Dataset created successfully!\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'chunk', 'question', 'id'],\n",
      "    num_rows: 279\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "print(\"\\nCreating Hugging Face Dataset...\")\n",
    "if not dataset_list:\n",
    "    print(\"No data was generated. The dataset will be empty.\")\n",
    "\n",
    "try:\n",
    "    dataset = Dataset.from_list(dataset_list)\n",
    "    # Add an id column to the dataset\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    \n",
    "    print(\"\\nDataset created successfully!\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # You can save the dataset locally if you wish, for example:\n",
    "    # dataset.save_to_disk(\"my_generated_dataset\")\n",
    "    # print(\"\\nDataset saved to 'my_generated_dataset' folder.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the Hugging Face Dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6dd94b7-9c36-4013-ab1c-873581eac8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 24.20ba/s]\n",
      "Creating json from Arrow format: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 499.80ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle Dataset\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Split Dataset Into a 90/10 Train/Test split\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Save Datasets to Disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5f477-19d9-4141-9750-094851f7c046",
   "metadata": {},
   "source": [
    "## Base Model Evaluation & Matryoshka Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c23feae-2df4-448b-9717-b646bf46a3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hugging Face model ID\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Loading via SentenceTransformer\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef32d91-35fe-44e5-ae62-dc7001d4b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets from their respective JSON files\n",
    "# These contain pairs of questions (anchors) and text chunks (positives)\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "\n",
    "# Combine train and test datasets into a single corpus\n",
    "# This ensures we have all possible text chunks available for retrieval evaluation\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert datasets into dictionary format required by the InformationRetrievalEvaluator\n",
    "# corpus: maps corpus IDs to their text chunks (documents)\n",
    "# Format: {corpus_id: text_chunk}\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"chunk\"])\n",
    ")\n",
    "\n",
    "# queries: maps query IDs to their questions\n",
    "# Format: {query_id: question_text}\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"question\"])\n",
    ")\n",
    "\n",
    "# Create a mapping between queries and their relevant documents\n",
    "# This tells the evaluator which documents are correct matches for each query\n",
    "relevant_docs = {}\n",
    "for q_id, global_chunk_id in zip(test_dataset[\"id\"], test_dataset[\"chunk_id\"]):\n",
    "    # Initialize empty list for each query if not already present\n",
    "    if q_id not in relevant_docs:\n",
    "        relevant_docs[q_id] = []\n",
    "\n",
    "    # Find all corpus entries that share the same global_chunk_id\n",
    "    # This handles cases where multiple questions can refer to the same text chunk\n",
    "    matching_corpus_ids = [\n",
    "        cid for cid, chunk in zip(corpus_dataset[\"id\"], corpus_dataset[\"chunk_id\"])\n",
    "        if chunk == global_chunk_id\n",
    "    ]\n",
    "    # Add the matching corpus IDs to the relevant documents for this query\n",
    "    relevant_docs[q_id].extend(matching_corpus_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f57052-dc3a-4236-9780-8742c84c9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of interest\n",
    "matryoshka_dimensions = [384] # Important: large to small\n",
    "\n",
    "# Create empty list to hold evaluators\n",
    "matryoshka_evaluators = []\n",
    "\n",
    "# Create an evaluator for each above dimension\n",
    "for dim in matryoshka_dimensions:\n",
    "    # Define the evaluator\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to the respective dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "        accuracy_at_k = [1, 3],\n",
    "        precision_recall_at_k = [1, 3],\n",
    "        mrr_at_k = [3],\n",
    "        ndcg_at_k = [3],\n",
    "        map_at_k = [3]\n",
    "    )\n",
    "    # Add to list\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "# Create a sequential evaluator\n",
    "# Able to run all our dimension specific InformationRetrievalEvaluators sequentially.\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43bce2c-48b8-4b40-9674-c4552f4be9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Evaluation Results\n",
      "-------------------------------------------------------------------------------------\n",
      "Metric                  384d\n",
      "-------------------------------------------------------------------------------------\n",
      "==ndcg@3==             0.6786 \n",
      "mrr@3                  0.6786 \n",
      "map@3                  0.6786 \n",
      "accuracy@1             0.6786 \n",
      "accuracy@3             0.6786 \n",
      "precision@1            0.6786 \n",
      "precision@3            0.6786 \n",
      "recall@1               0.2262 \n",
      "recall@3               0.6786 \n",
      "-------------------------------------------------------------------------------------\n",
      "seq_score: 0.678571\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "base_results = evaluator(model)\n",
    "\n",
    "# Print header\n",
    "print(\"\\nBase Model Evaluation Results\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Metric':15} {'384d':>12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# List of metrics to display\n",
    "metrics = [\n",
    "    'ndcg@3',\n",
    "    'mrr@3',\n",
    "    'map@3',\n",
    "    'accuracy@1',\n",
    "    'accuracy@3',\n",
    "    'precision@1',\n",
    "    'precision@3',\n",
    "    'recall@1',\n",
    "    'recall@3',\n",
    "]\n",
    "\n",
    "# Print each metric\n",
    "for metric in metrics:\n",
    "    values = []\n",
    "    for dim in matryoshka_dimensions:\n",
    "        key = f\"dim_{dim}_cosine_{metric}\"\n",
    "        values.append(base_results[key])\n",
    "\n",
    "    # Highlight NDCG@10\n",
    "    metric_name = f\"=={metric}==\" if metric == \"ndcg@3\" else metric\n",
    "    print(f\"{metric_name:15}\", end=\"  \")\n",
    "    for val in values:\n",
    "        print(f\"{val:12.4f}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# Print sequential score\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'seq_score:'} {base_results['sequential_score']:1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a2907-db0d-4ec8-859c-98a0ba32c1f9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00506370-514e-44be-a77c-534312a9b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with SDPA for using Flash Attention 2\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"ModernBERT Embed base Legal Matryoshka\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "208d6f98-5b05-4996-86db-e3ee92ea726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Loss\n",
    "base_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Matryoshka Loss Wrapper\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, base_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2827af0e-c813-45f5-a83c-2292b065746b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"saved-model\", # output directory and hugging face model ID\n",
    "    num_train_epochs=4,                                        # number of epochs\n",
    "    per_device_train_batch_size=32,                            # train batch size\n",
    "    gradient_accumulation_steps=16,                            # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,                             # evaluation batch size\n",
    "    warmup_ratio=0.1,                                          # warmup ratio\n",
    "    learning_rate=2e-5,                                        # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                                 # use fused adamw optimizer\n",
    "    tf32=True,                                                 # use tf32 precision\n",
    "    bf16=True,                                                 # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,                 # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",                                     # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                                     # save after each epoch\n",
    "    logging_steps=10,                                          # log every 10 steps\n",
    "    save_total_limit=3,                                        # save only the last 3 models\n",
    "    load_best_model_at_end=True,                               # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_dim_384_cosine_map@3\",       # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    "    report_to=\"none\"                                           # Turning off training logging for now, input 'wandb' etc. if desired.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9765c16-0dae-427d-b89b-88abc9c03c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset.select_columns(\n",
    "        [\"chunk\", \"question\"]\n",
    "    ),  # training dataset\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8e01343-5f11-4f68-8999-d1c353bda2a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column 'question' is at index 1, whereas a column with this name is usually expected at index 0. Note that the column order can be important for some losses, e.g. MultipleNegativesRankingLoss will always consider the first column as the anchor and the second as the positive, regardless of the dataset column names. Consider renaming the columns to match the expected order, e.g.:\n",
      "dataset = dataset.select_columns(['question', 'answer'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dim 384 Cosine Accuracy@1</th>\n",
       "      <th>Dim 384 Cosine Accuracy@3</th>\n",
       "      <th>Dim 384 Cosine Precision@1</th>\n",
       "      <th>Dim 384 Cosine Precision@3</th>\n",
       "      <th>Dim 384 Cosine Recall@1</th>\n",
       "      <th>Dim 384 Cosine Recall@3</th>\n",
       "      <th>Dim 384 Cosine Ndcg@3</th>\n",
       "      <th>Dim 384 Cosine Mrr@3</th>\n",
       "      <th>Dim 384 Cosine Map@3</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the best model based on our eval_dim_128_cosine_ndcg@10 criteria\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e9f2c-b4df-4fa7-8a9e-630407d26b1f",
   "metadata": {},
   "source": [
    "### Evaluating Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5c7ede0-4a8b-4288-8d93-c1ce0c5e8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "ft_results = evaluator(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a05b70a-1c18-4cfe-ab03-e0dc58048816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned Model Evaluation Results\n",
      "-------------------------------------------------------------------------------------\n",
      "Metric                  384d\n",
      "-------------------------------------------------------------------------------------\n",
      "==ndcg@3==             0.7500 \n",
      "mrr@3                  0.7500 \n",
      "map@3                  0.7500 \n",
      "accuracy@1             0.7500 \n",
      "accuracy@3             0.7500 \n",
      "precision@1            0.7500 \n",
      "precision@3            0.7500 \n",
      "recall@1               0.2500 \n",
      "recall@3               0.7500 \n",
      "-------------------------------------------------------------------------------------\n",
      "seq_score: 0.750000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Print header\n",
    "print(\"\\nFine-tuned Model Evaluation Results\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Metric':15} {'384d':>12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# List of metrics to display\n",
    "metrics = [\n",
    "    'ndcg@3',\n",
    "    'mrr@3',\n",
    "    'map@3',\n",
    "    'accuracy@1',\n",
    "    'accuracy@3',\n",
    "    'precision@1',\n",
    "    'precision@3',\n",
    "    'recall@1',\n",
    "    'recall@3',\n",
    "]\n",
    "\n",
    "# Print each metric\n",
    "for metric in metrics:\n",
    "    values = []\n",
    "    for dim in matryoshka_dimensions:\n",
    "        key = f\"dim_{dim}_cosine_{metric}\"\n",
    "        values.append(ft_results[key])\n",
    "\n",
    "    # Highlight NDCG@10\n",
    "    metric_name = f\"=={metric}==\" if metric == \"ndcg@3\" else metric\n",
    "    print(f\"{metric_name:15}\", end=\"  \")\n",
    "    for val in values:\n",
    "        print(f\"{val:12.4f}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# Print sequential score\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'seq_score:'} {ft_results['sequential_score']:1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
